{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1498, which is longer than the specified 128\n",
      "Created a chunk of size 2516, which is longer than the specified 128\n",
      "Created a chunk of size 369, which is longer than the specified 128\n",
      "Created a chunk of size 177, which is longer than the specified 128\n",
      "Created a chunk of size 163, which is longer than the specified 128\n",
      "Created a chunk of size 129, which is longer than the specified 128\n",
      "Created a chunk of size 184, which is longer than the specified 128\n",
      "Created a chunk of size 409, which is longer than the specified 128\n",
      "Created a chunk of size 164, which is longer than the specified 128\n",
      "Created a chunk of size 175, which is longer than the specified 128\n",
      "Created a chunk of size 191, which is longer than the specified 128\n",
      "Created a chunk of size 137, which is longer than the specified 128\n",
      "Created a chunk of size 455, which is longer than the specified 128\n",
      "Created a chunk of size 154, which is longer than the specified 128\n",
      "Created a chunk of size 180, which is longer than the specified 128\n",
      "Created a chunk of size 150, which is longer than the specified 128\n",
      "Created a chunk of size 465, which is longer than the specified 128\n",
      "Created a chunk of size 233, which is longer than the specified 128\n",
      "Created a chunk of size 187, which is longer than the specified 128\n",
      "Created a chunk of size 214, which is longer than the specified 128\n",
      "Created a chunk of size 465, which is longer than the specified 128\n",
      "Created a chunk of size 279, which is longer than the specified 128\n",
      "Created a chunk of size 315, which is longer than the specified 128\n",
      "Created a chunk of size 253, which is longer than the specified 128\n",
      "Created a chunk of size 148, which is longer than the specified 128\n",
      "Created a chunk of size 158, which is longer than the specified 128\n",
      "Created a chunk of size 271, which is longer than the specified 128\n",
      "Created a chunk of size 247, which is longer than the specified 128\n",
      "Created a chunk of size 589, which is longer than the specified 128\n",
      "Created a chunk of size 314, which is longer than the specified 128\n",
      "Created a chunk of size 322, which is longer than the specified 128\n",
      "Created a chunk of size 454, which is longer than the specified 128\n",
      "Created a chunk of size 136, which is longer than the specified 128\n",
      "Created a chunk of size 155, which is longer than the specified 128\n",
      "Created a chunk of size 201, which is longer than the specified 128\n",
      "Created a chunk of size 205, which is longer than the specified 128\n",
      "Created a chunk of size 225, which is longer than the specified 128\n",
      "Created a chunk of size 257, which is longer than the specified 128\n",
      "Created a chunk of size 196, which is longer than the specified 128\n",
      "Created a chunk of size 196, which is longer than the specified 128\n",
      "Created a chunk of size 333, which is longer than the specified 128\n",
      "Created a chunk of size 302, which is longer than the specified 128\n",
      "Created a chunk of size 377, which is longer than the specified 128\n",
      "Created a chunk of size 251, which is longer than the specified 128\n",
      "Created a chunk of size 498, which is longer than the specified 128\n",
      "Created a chunk of size 373, which is longer than the specified 128\n",
      "Created a chunk of size 397, which is longer than the specified 128\n",
      "Created a chunk of size 375, which is longer than the specified 128\n",
      "Created a chunk of size 305, which is longer than the specified 128\n",
      "Created a chunk of size 388, which is longer than the specified 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./Big Star Collectibles/.DS_Store, length: 6065 characters\n",
      "Got 4 chunks from ./Big Star Collectibles/.DS_Store\n",
      "Reading ./Big Star Collectibles/Product Mantras.txt, length: 5291 characters\n",
      "Got 40 chunks from ./Big Star Collectibles/Product Mantras.txt\n",
      "Reading ./Big Star Collectibles/Disclaimer.txt, length: 428 characters\n",
      "Got 1 chunks from ./Big Star Collectibles/Disclaimer.txt\n",
      "Reading ./Big Star Collectibles/Our Story.txt, length: 875 characters\n",
      "Got 3 chunks from ./Big Star Collectibles/Our Story.txt\n",
      "Reading ./Big Star Collectibles/FAQ.txt, length: 3162 characters\n",
      "Got 20 chunks from ./Big Star Collectibles/FAQ.txt\n",
      "Reading ./Big Star Collectibles/Our Team.txt, length: 1539 characters\n",
      "Got 4 chunks from ./Big Star Collectibles/Our Team.txt\n",
      "Reading ./Big Star Collectibles/Careers.txt, length: 1320 characters\n",
      "Got 10 chunks from ./Big Star Collectibles/Careers.txt\n",
      "Reading ./Big Star Collectibles/Contact Us.txt, length: 256 characters\n",
      "Got 1 chunks from ./Big Star Collectibles/Contact Us.txt\n",
      "Reading ./Big Star Collectibles/What We Do.txt, length: 1203 characters\n",
      "Got 9 chunks from ./Big Star Collectibles/What We Do.txt\n",
      "Reading ./Big Star Collectibles/Product Descriptions.txt, length: 3951 characters\n",
      "Got 22 chunks from ./Big Star Collectibles/Product Descriptions.txt\n",
      "Reading ./Big Star Collectibles/Mission & Values.txt, length: 521 characters\n",
      "Got 5 chunks from ./Big Star Collectibles/Mission & Values.txt\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Step 1: Build Vector Store from Documents\n",
    "# ------------------------------\n",
    "\n",
    "# Use HuggingFaceEmbeddings with a lightweight model (no API key needed)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Specify the directory containing your text files\n",
    "data_dir = \"./Big Star Collectibles\"\n",
    "files = os.listdir(data_dir)\n",
    "documents = []\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    # Skip items that are not files (directories, etc.)\n",
    "    if not os.path.isfile(file_path):\n",
    "        continue\n",
    "\n",
    "    # Open file with errors ignored to avoid UnicodeDecodeError\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        text = f.read().strip()\n",
    "    print(f\"Reading {file_path}, length: {len(text)} characters\")\n",
    "    if not text:\n",
    "        print(f\"Warning: {file_path} is empty.\")\n",
    "        continue\n",
    "\n",
    "    # Split text into manageable chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32, separator=\"\\n\")\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"Got {len(chunks)} chunks from {file_path}\")\n",
    "    \n",
    "    # Wrap each chunk into a Document object\n",
    "    for chunk in chunks:\n",
    "        documents.append(Document(page_content=chunk))\n",
    "\n",
    "if not documents:\n",
    "    raise ValueError(\"No text chunks found. Please check your file reading and splitting logic.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a FAISS vector store from the documents\n",
    "vector_store = FAISS.from_documents(documents, embedding=embeddings)\n",
    "print(\"Vector store created successfully!\")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/var/folders/k0/h7lvf47d4z31ztdbyt2m05rr0000gn/T/ipykernel_4848/2852380164.py:26: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipeline_llm)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Step 2: Setup the Prompt and LLM\n",
    "# ------------------------------\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define a Hugging Face LLM using a transformers pipeline\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# Use GPT-2, but increase max_length and set max_new_tokens to allow for additional output.\n",
    "# You can adjust these parameters as needed.\n",
    "pipeline_llm = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"gpt2\", \n",
    "    max_length=200,          # Increase overall max_length\n",
    "    max_new_tokens=50,       # Allow 50 new tokens to be generated\n",
    "    truncation=True          # Explicitly enable truncation\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipeline_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll also import the output parser to clean up the result\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: Retrieve Context and Generate an Answer\n",
    "# ------------------------------\n",
    "\n",
    "def get_answer(question: str) -> str:\n",
    "    # Retrieve relevant documents for the question\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "    # Format the prompt with the question and retrieved context\n",
    "    formatted_prompt = prompt.format(question=question, context=context)\n",
    "    # Generate the LLM output\n",
    "    llm_output = llm(formatted_prompt)\n",
    "    # Parse the output to get a clean string answer\n",
    "    answer = StrOutputParser().parse(llm_output)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k0/h7lvf47d4z31ztdbyt2m05rr0000gn/T/ipykernel_4848/1146759965.py:10: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n",
      "/var/folders/k0/h7lvf47d4z31ztdbyt2m05rr0000gn/T/ipykernel_4848/1146759965.py:15: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm_output = llm(formatted_prompt)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=50) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chain output:\n",
      "Human: You are a helpful assistant. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: What is Big Star Collectibles about? \n",
      "Context: What are Big Star Collectibles products printed on?\n",
      "From the start, Big Star Collectibles has been about quality rather than quantity. We strive to give our customers the absolute best of the best, and we also acknowledge that creating limited editions enhances and adds to the value of each product, now and in the future.\n",
      "Big Star Collectibles has grown over the years to include memorabilia, contests, events, appraisals, and consultation services.\n",
      "For a fee, our experts can assist you in finding a particular Big Star Collectibles item that you have been looking for. Big Star Collectibles can also broker sales and trades among our customers. \n",
      "Answer:\n",
      "Big Star Collectibles is committed to delivering your personal Big Star Collectibles to your door whenever you're looking for a particular item, and can even trade their terms during your order. To learn more about our customers' deals, please visit www.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Example Usage\n",
    "# ------------------------------\n",
    "\n",
    "question = \"What is Big Star Collectibles about?\"\n",
    "result = get_answer(question)\n",
    "print(\"\\nChain output:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
